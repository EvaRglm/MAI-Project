# -*- coding: utf-8 -*-
"""Demo of GLIP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12x7v-_miN7-SRiziK3Cx4ffJzstBJNqb

# GLIP

Welcome to the demo notebook for GLIP https://github.com/microsoft/GLIP!
"""

# Install CUDA 10.2; newer versions of CUDA may fail
# !apt-get update -y
# !apt-get --purge remove "*cublas*" "cuda*" "nsight*"
# !nvcc --version

# !wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin
# !mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600
# !wget https://developer.download.nvidia.com/compute/cuda/10.2/Prod/local_installers/cuda-repo-ubuntu1804-10-2-local-10.2.89-440.33.01_1.0-1_amd64.deb
# !dpkg -i cuda-repo-ubuntu1804-10-2-local-10.2.89-440.33.01_1.0-1_amd64.deb
# !apt-key add /var/cuda-repo-10-2-local-10.2.89-440.33.01/7fa2af80.pub
# !apt-get update
# !apt-get -y install cuda

# Commented out IPython magic to ensure Python compatibility.
# Install Environments. This will take a few minutes. Please be patient ;)
# ! nvidia-smi
# ! git clone https://github.com/microsoft/GLIP.git
# # % cd GLIP
# ! git checkout c663d9db8a503e04c6b76cd2e14152bab775d28a
# ! pip install torch==1.9.0 torchvision  torchaudio
# ! pip install einops shapely timm yacs tensorboardX ftfy prettytable pymongo
# ! pip install transformers
# ! python setup.py build develop --user
# ! mkdir MODEL

import matplotlib.pyplot as plt
import matplotlib.pylab as pylab

import requests
from io import BytesIO
from PIL import Image
import numpy as np
pylab.rcParams['figure.figsize'] = 20, 12
from maskrcnn_benchmark.config import cfg
from maskrcnn_benchmark.engine.predictor_glip import GLIPDemo

def load(url):
    """
    Given an url of an image, downloads the image and
    returns a PIL image
    """
    response = requests.get(url)
    pil_image = Image.open(BytesIO(response.content)).convert("RGB")
    # convert to BGR format
    image = np.array(pil_image)[:, :, [2, 1, 0]]
    return image

def imshow(img, caption, i):
    plt.imshow(img[:, :, [2, 1, 0]])
    plt.axis("off")
    plt.figtext(0.5, 0.09, caption, wrap=True, horizontalalignment='center', fontsize=20)
    plt.savefig("result_"+i+".png")

# Use this command for evaluate the GLPT-T model
# ! wget https://penzhanwu2bbs.blob.core.windows.net/data/GLIPv1_Open/models/glip_tiny_model_o365_goldg_cc_sbu.pth -O MODEL/glip_tiny_model_o365_goldg_cc_sbu.pth
config_file = "configs/pretrain/glip_Swin_T_O365_GoldG.yaml"
weight_file = "MODEL/glip_tiny_model_o365_goldg_cc_sbu.pth"

# Use this command to evaluate the GLPT-L model
# ! wget https://penzhanwu2bbs.blob.core.windows.net/data/GLIPv1_Open/models/glip_large_model.pth -O MODEL/glip_large_model.pth
# config_file = "configs/pretrain/glip_Swin_L.yaml"
# weight_file = "MODEL/glip_large_model.pth"

# update the config options with the config file
# manual override some options
cfg.local_rank = 0
cfg.num_gpus = 1
cfg.merge_from_file(config_file)
cfg.merge_from_list(["MODEL.WEIGHT", weight_file])
cfg.merge_from_list(["MODEL.DEVICE", "cuda"])

glip_demo = GLIPDemo(
    cfg,
    min_image_size=800,
    confidence_threshold=0.7,
    show_mask_heatmaps=False
)

"""Next, we retrieve an image on which we wish to test the model. Here, we use an image from the validation set of COCO"""

image = load('http://farm4.staticflickr.com/3693/9472793441_b7822c00de_z.jpg')
caption = 'bobble heads on top of the shelf'
result, _ = glip_demo.run_on_web_image(image, caption, 0.5)
print(caption)
imshow(result, caption, "1")

image = load('http://farm4.staticflickr.com/3693/9472793441_b7822c00de_z.jpg')
caption = 'sofa . remote . dog . person . car . sky . plane .' # the caption can also be the simple concatonation of some random categories.
result, _ = glip_demo.run_on_web_image(image, caption, 0.5)
print(caption)
imshow(result, caption, "2")